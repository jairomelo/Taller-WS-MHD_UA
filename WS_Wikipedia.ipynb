{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping: lo básico\n",
    "\n",
    "\n",
    "![](https://drive.google.com/uc?id=1cmCRKtVedlfkXlQHKk3CUzk-1MLlMAuu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CpWFMXTSULEG"
   },
   "source": [
    "# Web Scraping Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t-QhbThwZ3eE"
   },
   "source": [
    "BeautifulSoup es una librería muy útil para hacer Web Scraping. \n",
    "\n",
    "La manera de utilizarla es bastante simple:\n",
    "\n",
    "En primer lugar, necesitamos importar el **módulo** de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vDJcexy7UGoR"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k1-dAI5YaRWz"
   },
   "source": [
    "Sin embargo, Python no es un navegador Web, así que no podemos \"llamar\" directamente la página sin un módulo que se comunique con el servidor. \n",
    "\n",
    "Para nuestro ejercicio utilizaremos `urllib.request`, un módulo relativamente simple para comunicarse con páginas web con el protocolo HTTP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pP80aKeIUYae"
   },
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yesHYaivge7B"
   },
   "source": [
    "Ahora, vamos a guardar en una **variable** la dirección URL que queremos recuperar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RUOWvJMPgmEA"
   },
   "outputs": [],
   "source": [
    "host = \"https://es.wikipedia.org/wiki/Colombia\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HvfKr0Dsg1c4"
   },
   "source": [
    "El paso siguiente consiste en \"preparar la sopa\". Para ello utilizaremos dos funciones: una para recuperar la página Web y otra para leer el código HTML. \n",
    "\n",
    "Guardaremos cada función en variables, la primera la llamaremos `salsa` y a la segunda `sopa`.\n",
    "\n",
    "*Observa como utilizamos el módulo `urllib.request` al cual le pasamos una **función** (`.urlopen`) e incluimos un **parámetro** dentro del paréntesis, el cual es la variable que contiene nuestra URL*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZDViruToUw79"
   },
   "outputs": [],
   "source": [
    "salsa = urllib.request.urlopen(host).read() \n",
    "sopa =  BeautifulSoup(salsa, 'html.parser')\n",
    "\n",
    "print(sopa)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jg9uqdLfnyOm"
   },
   "source": [
    "El problema, como podrán observar, es que obtenemos todo el código HTML de la página, lo cual es igual a simplemente guardar el sitio. \n",
    "\n",
    "Lo que queremos es poder escoger partes específicas de la página para luego guardarlas o aplicarles algún análisis. \n",
    "\n",
    "En ese caso, necesitamos identificar en qué **etiquetas** se guarda la información. Empezaremos con la más sencilla, el texto.\n",
    "\n",
    "La manera más recomendable para hacer este ejercicio consiste en utilizar la herramienta inspeccionar, disponibles en Google Chrome y Mozilla.\n",
    "\n",
    "Así, escogemos el contenido que queremos recuperar y con el click derecho seleccionamos 'Inspeccionar' (en Chrome) o 'Inspeccionar elemento' (en Mozilla)\n",
    "\n",
    "![](https://drive.google.com/uc?id=1tc9TZcG4nZo5VdPF_OXvYrgX-lFRHGdM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8-S28eFrZuO3"
   },
   "source": [
    "Si observas detenidamente verás que alrededor de la palabra 'Colombia' hay dos **etiquetas**. Así se llaman en HTML y otros lenguajes de marcado de texto a los elementos que indican una función que debe cumplir el texto al momento de ser desplegado en el navegador, asimismo, sirven para señalar información propia del sitio Web como el título, la codificación, los metadatos para los motores de búsqueda, los recursos externos como JavaScript o CSS, entre otros.\n",
    "\n",
    "Se identifican de manera sencilla porque se incluyen entre corchetes angulares así: `<etiqueta></etiqueta>`\n",
    "\n",
    "La estructura del marcado se hace de manera 'anidada', así una palabra en negrilla está anidada en una etiqueta superior, que sería texto. Para nuestro ejemplo, la estructura iría de la siguiente manera\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aIf2u6vTZwaO"
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<p>\n",
    "  <b>Colombia</b>, oficialmente <b>República de Colombia</b>...\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ho9REeFiarGj"
   },
   "source": [
    "Así, sabemos que la etiqueta para el texto es `<p>` ('párrafo') y otras como `<b>` ('negrilla' o 'bold'), `<a>` ('enlace') o `<i>` ('itálica' o 'cursiva') están anidadas a `<p>`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I0AaZcaHcNPD"
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<p>\n",
    "  Este es un texto de párrafo, que puedo incluir una <b>negrilla</b>, un <a href=\"https://www.google.com/\" target =\"_blank\">enlace</a>, o simplemente un texto en <i>cursiva</i>.\n",
    "</p>\n",
    "\n",
    "<b>Asimismo, puedo <i>anidar</i> etiquetas que no están en <p>párrafo</p>. Pero su comportamiento es completamente diferente.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0PFfhMhscxsP"
   },
   "source": [
    "Por otra parte, existen otras etiquetas que indican divisiones más grandes y su etiqueta, como era de suponerse, es `<div>`. Si pasas el cursor sobre los diferentes elementos del panel de inspección, verás como se resaltan esas divisiones y se indica el nombre con el cual se identifican.\n",
    "\n",
    "![](https://drive.google.com/uc?id=1YXN4MqF_yZC4LA-FKg6XtTtqJMkS51zG)\n",
    "\n",
    "Así, la etiqueta que alberga el contenido principal del artículo tiene esta forma:\n",
    "\n",
    "`<div id=\"bodyContent\" class=\"mw-body-content\"></div>`\n",
    "\n",
    "Donde `id` es el **atributo** que identifica a la etiqueta, es único y no se repite en todo el documento. El atributo `class` remite a una clase en una hoja de estilos CSS. \n",
    "\n",
    "De manera simple funciona de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UgGIBZEiiJRr"
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<!-- A esto se le denomina hoja de estilos\n",
    "Con este código vamos a indicarle a la página\n",
    " que sólo los párrafos que estén en el 'div'\n",
    "tengan fondo amarillo.\n",
    "-->\n",
    "\n",
    "<style>\n",
    "  .mw-body-content p { background-color: yellow; }\n",
    "  .mw-body-content h3 { color: red; }\n",
    "</style>\n",
    "\n",
    "<!-- Este es el cuerpo del documento -->\n",
    "\n",
    "<h3>Este es un título normal</h3>\n",
    "<p>Este es un párrafo de primer nivel</p>\n",
    "\n",
    "<div id=\"bodyContent\" class=\"mw-body-content\">\n",
    "  <h3>Este título lleva otro color</h3>\n",
    "  <p>Este párrafo está dentro de un <code>div</code></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jHlyl7HQcQdX"
   },
   "source": [
    "> Obviamente cubrimos una mínima parte del etiquetado HTML y sus atributos. Por eso es recomendable profundizar en el lenguaje para comprender mejor cómo se estructura un sitio Web.\n",
    "\n",
    "> Un buen lugar para empezar es https://www.w3schools.com/html/. En español, un recurso interesante es https://www.w3.org/Style/Examples/011/firstcss.es.html.\n",
    "\n",
    "> Para practicar es recomendable utilizar https://codepen.io/\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8dGluuKTn5BJ"
   },
   "source": [
    "# Web Scraping a una página\n",
    "\n",
    "Empecemos con un ejercicio simple. Vamos a ejecutar el siguiente código y veamos cuál es el resultado (asegúrate de haber ejecutado las tres primeras celdas de código para que funcione este script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VuUNoLzwWGzM"
   },
   "outputs": [],
   "source": [
    "salsa = urllib.request.urlopen(host).read()\n",
    "sopa =  BeautifulSoup(salsa, 'html.parser')\n",
    "\n",
    "for texto in sopa.find_all('p'):\n",
    "\tprint(texto.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M4njQ01ro24Q"
   },
   "source": [
    "Expliquemos primero lo que hace este script:\n",
    "\n",
    "\n",
    "\n",
    "1.   Repetimos el paso de la celda [4] y guardamos en variables la `salsa` y la `sopa`\n",
    "2.   Usamos un **loop** para seleccionar todos los elementos de párrafo `('p')` en la `sopa`\n",
    "3.   Le pedimos a la consola que nos muestre el resultado **pero** solamente el texto (no el código HTML), por lo que pasamos la función `.text` para que nos recupere sólo el texto.\n",
    "\n",
    "El problema es que no nos muestra los encabezados, que se encuentran etiquetados como `<h1>`, `<h2>` y `<h3>`. Si pasamos el código para que nos recupere esos títulos obtenemos lo siguiente:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WLizQwVqrG8a"
   },
   "outputs": [],
   "source": [
    "# No necesitamos la `sopa` porque ya la tenemos hecha en la celda anterior.\n",
    "\n",
    "for texto in sopa.find_all(['h1','h2','h3']): # Reemplazamos la etiqueta de párrafo por la de título\n",
    "\tprint(texto.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cYchPkaUrP3e"
   },
   "source": [
    "Nota que incluimos una nueva forma de pasar nuestra información en la función `find_all()`. En este caso utilizamos una **lista**, que se indica mediante corchetes y cada elemento de la lista se separa por comas `['1', '2', '3'... 'n']`. \n",
    "\n",
    "Así, no es necesario escribir un loop para cada encabezado sino podemos hacerlo para todos al mismo tiempo.\n",
    "\n",
    "Sin embargo, lo que queremos es el texto, tanto de los párrafos como de los títulos. Para ello utilizaremos el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_C4bGpult4lh"
   },
   "outputs": [],
   "source": [
    "for titulo in sopa.find_all(['h1','h2','h3']):\n",
    "\tprint(titulo.text)\n",
    "\tfor parrafo in titulo.next_siblings:\n",
    "\t\tif parrafo.name and parrafo.name.startswith('h'):\n",
    "\t\t\tbreak\n",
    "\t\tif parrafo.name == 'p':\n",
    "\t\t\tprint(parrafo.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-yw9kcnnupRH"
   },
   "source": [
    "Con este script recuperamos la información general del artículo incluyendo sus títulos y párrafos. Lo explico para que sepas cómo funciona:\n",
    "\n",
    "\n",
    "\n",
    "1.   Pasamos por un loop que nos recupere todos los elementos que contengan las etiquetas 'h1', 'h2' o 'h3'.\n",
    "2.   Le pedimos a python que nos imprima el texto de los encabezados.\n",
    "3.   Ahora, a cada encabezado le pedimos que nos identifique sus elementos 'hermanos'.\n",
    "4.   Tenemos ahora que pasar una condición lógica que diga: \"Si el elemento hermano es igual a 'p' entonces imprime el párrafo de texto\"\" (`if parrafo.name == 'p': print(parrafo.text)`). Pero antes debemos indicarle a Python que \"Si hay un elemento hermano y este elemento hermano empieza con 'h' entonces no haga nada\". Si no pasamos esta condición de esa manera solamente nos escribirá los párrafos después del primer título y no los demás títulos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tfjvGicI0i9d"
   },
   "source": [
    "# Web Scraping a múltiples páginas\n",
    "\n",
    "Obviamente, reducir el ejercicio a una sola página no tiene mucho sentido. No es necesario hacer un programa si podemos simplemente guardar la página o copiar el texto de una página Web.\n",
    "\n",
    "Por eso, el uso más extendido del Web Scraping consiste en recuperar la información de varias páginas con un solo script. \n",
    "\n",
    "Vamos a proponernos dos ejercicios:\n",
    "\n",
    "1.   Recuperar el texto de todas las páginas de una categoría de Wikipedia.\n",
    "2.   Recuperar el texto de un artículo en todos los idiomas que tenga disponible.\n",
    "\n",
    "Vamos a utilizar la categoría [Países del mar Caribe](https://es.wikipedia.org/wiki/Categor%C3%ADa:Pa%C3%ADses_del_mar_Caribe) y el artículo [Colombia](https://es.wikipedia.org/wiki/Colombia) para estos ejercicios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xyJptcFM2h08"
   },
   "source": [
    "## Recuperar la información de una serie de páginas en una categoría\n",
    "\n",
    "Primero, vamos a aprovechar lo que ya conocemos y convertiremos el loop para extraer el texto en una **función**. Las funciones son muy útiles porque nos evitan tener que escribir el mismo código varias veces, además, en caso de error es más sencillo arreglar una sola función que buscar por todo el código cada vez que se repita.\n",
    "\n",
    "En Python las funciones se escriben así:\n",
    "\n",
    "```python\n",
    "def mi_funcion(parametros):\n",
    "        acciones...\n",
    "```\n",
    "\n",
    "De esta manera, nuestra función quedará así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SO_JbqIsCyVe"
   },
   "outputs": [],
   "source": [
    "def ws_texto(miurl):\n",
    "\tsalsa2 = urllib.request.urlopen(miurl).read()\n",
    "\tsopa2 =  BeautifulSoup(salsa2, 'html.parser')\n",
    "\tfor titulo in sopa2.find_all(['h1','h2','h3']):\n",
    "\t\tprint(titulo.text)\n",
    "\t\tfor parrafo in titulo.next_siblings:\n",
    "\t\t\tif parrafo.name and parrafo.name.startswith('h'):\n",
    "\t\t\t\tbreak\n",
    "\t\t\tif parrafo.name == 'p':\n",
    "\t\t\t\tprint(parrafo.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8i_VOGUWEi24"
   },
   "source": [
    "Lo que hace nuestra función es que con cada URL (`miurl`) prepara una nueva `salsa2` y una nueva `sopa2` para extraer de cada enlace los títulos y el texto. \n",
    "\n",
    "Ya que hemos construido nuestra función, pasamos a encontrar los enlaces en la categoría. Como vemos, la página [Países del mar Caribe](https://es.wikipedia.org/wiki/Categor%C3%ADa:Pa%C3%ADses_del_mar_Caribe) está dividida en tres celdas: la primera contiene un mapa y el enlace a Wikimedia Commons, la segunda es un listado de subcategorías, la tercera contiene las páginas en la categoría «Países del mar Caribe». \n",
    "\n",
    "Concentraremos la búsqueda en esta última. Para ello, volvemos al inspector y allí podemos identificar que el listado de las páginas se encuentra en el div con el id `mw-pages`. \n",
    "\n",
    "![](https://drive.google.com/uc?id=1KJo9nLULPGjhrUVMMFoEo0UuRfqofxIv)\n",
    "\n",
    "Como requerimos buscar solamente los enlaces, aprovechamos que estos se marcan de manera sencilla con la etiqueta `<a>` y le pedimos a BeautifulSoup que nos busque los enlaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5KuX15MbEhyX"
   },
   "outputs": [],
   "source": [
    "host = \"https://es.wikipedia.org/wiki/Categor%C3%ADa:Pa%C3%ADses_del_mar_Caribe\"\n",
    "\n",
    "salsa = urllib.request.urlopen(host).read()\n",
    "sopa =  BeautifulSoup(salsa, 'html.parser')\n",
    "\n",
    "for contenido in sopa.select('#mw-pages a'):\n",
    "\tprint(contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H1Vq4PCdI6lh"
   },
   "source": [
    "El problema con este código es que en el momento de pasarlo por nuestra función saldrá un error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v7EU4NLIJDXB"
   },
   "outputs": [],
   "source": [
    "for contenido in sopa.select('#mw-pages a'):\n",
    "\tws_texto(contenido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UGOl5sGlJbba"
   },
   "source": [
    "El problema es que nuestro código muestra los enlaces de la siguiente manera:\n",
    "\n",
    "```html\n",
    "<a href=\"/wiki/Antigua_y_Barbuda\" title=\"Antigua y Barbuda\">Antigua y Barbuda</a>\n",
    "\n",
    "```\n",
    "\n",
    "Pero si copias ese texto en un navegador no te conduce al sitio. \n",
    "\n",
    "Necesitamos que ese texto se vea así:\n",
    "\n",
    "\n",
    "```html\n",
    "\"https://es.wikipedia.org/wiki/Antigua_y_Barbuda\"\n",
    "\n",
    "```\n",
    "\n",
    "Para ello necesitamos hacer dos cosas:\n",
    "\n",
    "1. Vamos a separar el enlace en \"host\" y \"ruta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GCa6T-XXKnWc"
   },
   "outputs": [],
   "source": [
    "host = \"https://es.wikipedia.org\"\n",
    "ruta = \"/wiki/Categor%C3%ADa:Pa%C3%ADses_del_mar_Caribe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "clKnI_BGK16U"
   },
   "source": [
    "Luego, creamos la URL uniendo las dos partes, para lo cual utilizaremos la función `format()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p8o8LmD7K81s"
   },
   "outputs": [],
   "source": [
    "url = \"{}{}\".format(host, ruta)\n",
    "\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WqLi0eeELGPL"
   },
   "source": [
    "Y rehacemos nuestra sopa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "18ke-W_GLZMx"
   },
   "outputs": [],
   "source": [
    "salsa = urllib.request.urlopen(url).read() # cambiamos el enlace de 'host' a 'url'\n",
    "sopa =  BeautifulSoup(salsa, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DtuRE06oLnUB"
   },
   "source": [
    "2. Vamos a seleccionar solamente la ruta del enlace que se encuentra en el atributo `href=\"/wiki/Antigua_y_Barbuda\"` Y uniremos esta ruta con la dirección del 'host' para construir nuestras URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ACnShNfzLmPx"
   },
   "outputs": [],
   "source": [
    "for contenido in sopa.select('#mw-pages a'):\n",
    "\tobtener = \"{}{}\".format(host, contenido['href'])\n",
    "\tprint(obtener)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zpwWCnBulnAZ"
   },
   "source": [
    "Como verás, ahora el listado consiste en enlaces bien definidos. De hecho, puedes hacer click sobre cualquiera de ellos y verás que abre la página sin problemas. \n",
    "Esta es otra de las ventajas de este método. Como no haces un listado manual, si Wikipedia elimina o agrega nuevas páginas podrás acceder a los enlaces sin tener que lidiar con enlaces rotos.\n",
    "\n",
    "El tercer paso consistirá en pasar nuestra función para cada enlace, con lo cual obtendremos el texto de cada página:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "px6FXahylnAh"
   },
   "outputs": [],
   "source": [
    "for contenido in sopa.select('#mw-pages a'):\n",
    "\tobtener = \"{}{}\".format(host, contenido['href'])\n",
    "\tws_texto(obtener)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "25v-k0QolnBQ"
   },
   "source": [
    "Pero imprimir el texto en la pantalla no es de mucha utilidad, lo que querríamos sería tener el texto para luego analizarlo con otra librería o con otro software. Lo que vamos a hacer entonces será crear un directorio para guardar nuestros resultados y un archivo de texto para cada página:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EJAlTTMQlnB7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "directorio = '{}/descargas/'.format(path)\n",
    "\n",
    "if not os.path.exists(directorio):\n",
    "\tos.makedirs(directorio)\n",
    "\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIfGWldNlnD0"
   },
   "source": [
    "Modificamos la función para que en lugar de imprimir cree un archivo de texto. Vamos a aprovechar el módulo `codecs` para garantizar que el resultado esté codificado adecuadamente en `utf-8` y no tengamos problemas con las tildes y diéresis.\n",
    "\n",
    "Para crear un archivo necesitamos utilizar la función `codecs.open('nombredelarchivo.txt')`, además, necesitamos agregar los parámetros `\"a\"`, para que cada título y párrafo se añadan al archivo de texto, y `\"utf-8\"` para evitar errores con la ortografía española.\n",
    "\n",
    "Para identificar cada archivo, creamos seleccionamos el título del artículo y lo añadimos como nombre del archivo.\n",
    "\n",
    "Además, para ayudar con la identificación del archivo vamos a agregar entre corchetes el idioma de la página. Así, el nombre del archivo quedará compuesto de tres componentes:\n",
    "\n",
    "- La ruta (directorio)\n",
    "- El lenguaje\n",
    "- El título\n",
    "\n",
    "Así, el artículo de, por ejemplo, Antigua y Barbuda, se llamará: `/directorio/[es]Antigua y Barbuda.txt` \n",
    "\n",
    "Para escribir el contenido utilizamos la función `write`, de tal manera que simplemente reemplazamos `print()` por `archiv.write()`.\n",
    "\n",
    "Añadimos un condicional para evitar que descarguemos dos veces la misma información `if not os.path.exists(archivo):`\n",
    "\n",
    "Nuestra función ahora se ve así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZbSjuD78lnEB"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "def ws_texto(miurl, directorio): # Nuestra función requiere dos parámetros: una url (`miurl`) y un directorio\n",
    "\tsalsa2 = urllib.request.urlopen(miurl).read()\n",
    "\tsopa2 =  BeautifulSoup(salsa2, 'html.parser')\n",
    "\tnombre = sopa2.find('h1', class_=\"firstHeading\") # Título del artículo\n",
    "\ttitulo = nombre.text\n",
    "\tlengua = extract = miurl.split('.')[-3].replace(\"https://\", \"\") # Manipulación de la ruta para extraer el indicador de leng.\n",
    "\tarchivo = \"{}/[{}]{}.txt\".format(directorio,lengua,titulo) # Construcción del nombre del archivo.\n",
    "\tif not os.path.exists(archivo): # Condicional -> Si no existe un archivo con ese nombre, escríbelo.\n",
    "\t\tarchiv = codecs.open(archivo, \"a\", \"utf-8\")\n",
    "\t\tfor titulo in sopa2.find_all(['h1','h2','h3']):\n",
    "\t\t\tarchiv.write(titulo.text)\n",
    "\t\t\tfor parrafo in titulo.next_siblings:\n",
    "\t\t\t\tif parrafo.name and parrafo.name.startswith('h'):\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\tif parrafo.name == 'p':\n",
    "\t\t\t\t\tarchiv.write(parrafo.text)\n",
    "\telse:\n",
    "\t\tprint(\"Ya existe el archivo\") # Mensaje si existe el archivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-gVFy01lnET"
   },
   "source": [
    "Y ahora podemos correr nuestro script para obtener los archivos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tmHlgbGMlnEs"
   },
   "outputs": [],
   "source": [
    "for contenido in sopa.select('#mw-pages a'):\n",
    "\tobtener = \"{}{}\".format(host, contenido['href'])\n",
    "\tws_texto(obtener, directorio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8LSC-A5mlnFT"
   },
   "source": [
    "En Google Colab podrás acceder a los archivos desde el menu de la columna izquierda, en la pestaña 'Archivos'\n",
    "\n",
    "![](https://drive.google.com/uc?id=164YRYz1XiP0kyTCh9NR00vkN0lNoYGh5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1OGjqHlagHjE"
   },
   "source": [
    "## Recuperar el texto de un artículo en todos los idiomas que tenga disponible\n",
    "\n",
    "Para recuperar esta información tendremos que repetir algunos pasos. Primero, vamos a identificar la etiqueta que contiene los enlaces del menú `En otros idiomas`, para lo cual utilizaremos nuevamente la herramienta de inspección:\n",
    "\n",
    "![](https://drive.google.com/uc?id=19BV56VAqQrvr3TxX48I0cgvJ1GtV8V7c)\n",
    "\n",
    "Como verás, cada enlace corresponde a una etiqueta `a` con el atributo `class=\"interlanguage-link-target\"`. Podemos observar también que el atributo `href` contiene un enlace completo, así que no será necesario \"dividir\" la url. \n",
    "\n",
    "Aprovecharemos nuestra función `ws_texto()` y solamente formaremos la ruta del directorio y de las url para recuperar la información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CXjja9iMgHjK"
   },
   "outputs": [],
   "source": [
    "url = \"https://es.wikipedia.org/wiki/Colombia\" # Guardamos en una variable la url\n",
    "\n",
    "salsa = urllib.request.urlopen(url).read()\n",
    "sopa =  BeautifulSoup(salsa, 'html.parser') # Hacemos la sopa\n",
    "\n",
    "path = os.getcwd()\n",
    "directorio = '{}/interlingua/'.format(path) # Cambiamos un poco la ruta del directorio para distinguirla del ejercicio anterior\n",
    "\n",
    "if not os.path.exists(directorio): # Creamos el directorio en caso de que no exista\n",
    "\tos.makedirs(directorio)\n",
    "    \n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Tv-6gtegHjX"
   },
   "source": [
    "Ahora, tendremos que recuperar primero la información de nuestra página `https://es.wikipedia.org/wiki/Colombia` para lo cual simplemente pasamos la url y la ruta del directorio por nuestra función `ws_texto()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N9i-UXyZgHjb"
   },
   "outputs": [],
   "source": [
    "ws_texto(url, directorio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x6nPulF8gHjm"
   },
   "source": [
    "Posteriormente, creamos un loop en el que seleccionamos la información de los enlaces a las páginas interlengua `'a', class_=\"interlanguage-link-target\"` y obtenemos el contenido del atributo `'href'`.\n",
    "\n",
    "Finalmente, pasamos cada enlace por nuestra función `ws_texto()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9IB0yjDYgHjs"
   },
   "outputs": [],
   "source": [
    "for contenido in sopa.find_all('a', class_=\"interlanguage-link-target\"):\n",
    "\tobtener = contenido['href']\n",
    "\tws_texto(obtener, directorio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recuperar información y tabularla\n",
    "\n",
    "Ahora vamos a hacer un ejercicio en el que recuperaremos la información de la tabla una página y la guardaremos en forma tabulada, es decir, en `csv`. Esta es una de las formas más comunes de guardar información que luego procederá a ser visualizada para su análisis. \n",
    "\n",
    "La forma más sencilla para manejar información tabular con Python es a través de la librería `pandas`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url =  'https://es.wikipedia.org/wiki/Anexo:Divisi%C3%B3n_pol%C3%ADtica_de_%C3%81frica'\n",
    "\n",
    "tabla = pd.read_html(url)[1]\n",
    "print(tabla)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos escoger ciertas columnas, por ejemplo `País`, `Superficie` y `Población`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tabla[['País', 'Superficie (km²)', 'Población (est. 2016)[2]​']] \n",
    "# Observa que el indicador de la columna debe ser EXACTAMENTE IGUAL al encabezado\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y podemos guardar la información de las tablas en un archivo `csv` para usarlo posteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un directorio para nuestro csv\n",
    "\n",
    "path = os.getcwd()\n",
    "directorio = '{}/csv/'.format(path) # Cambiamos un poco la ruta del directorio para distinguirla del ejercicio anterior\n",
    "\n",
    "if not os.path.exists(directorio): # Creamos el directorio en caso de que no exista\n",
    "\tos.makedirs(directorio)\n",
    "    \n",
    "os.listdir()\n",
    "\n",
    "# Guardamos nuestro archivo csv\n",
    "\n",
    "df.to_csv('{}pandas.csv'.format(directorio, encoding=\"utf-8\"))\n",
    "# En caso de utilizar el archivo csv con MSExcel será necesario codificarlo como `encoding=\"utf-8-sig\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular información no estructurada\n",
    "\n",
    "Uno de los ejercicios más comunes consiste en recuperar información no estructurada y tabularla para poder realizar análisis de la información. Lastimosamente, `pandas` solamente nos regresa la información que es posible encontrar en tablas, pero no nos permite seleccionar los enlaces o hacer iteraciones entre los elementos.\n",
    "\n",
    "Tratar con tablas es un poco más complejo que hacerlo con textos. Por ejemplo, si queremos recuperar los nombres de los países de las tablas disponibles en la página `Anexo:Países` de Wikipedia (https://es.wikipedia.org/wiki/Anexo:Pa%C3%ADses) vamos a tener que recuperar primero los enlaces y luego intentar descartar aquellos que no se refieran a nombres de países:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si recuperamos los enlaces de la tabla de manera simple obtendremos el siguiente resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"https://es.wikipedia.org\"\n",
    "ruta = \"/wiki/Anexo:Pa%C3%ADses\" \n",
    "url = \"{}{}\".format(host,ruta)\n",
    "\n",
    "salsa = urllib.request.urlopen(url).read()\n",
    "sopa =  BeautifulSoup(salsa, 'html.parser')\n",
    "\n",
    "for contenido in sopa.select('table a'): # Aquí seleccionamos todos los enlaces en una tabla\n",
    "    obtener = \"{}{}\".format(host, contenido['href'])\n",
    "    print(contenido.text)\n",
    "    print(obtener)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como verán, hay muchos resultados que no queremos: enlaces a imágenes, archivos, notas y sitios externos.\n",
    "Para seleccionar solamente los enlaces que necesitamos vamos a utilizar la condición `if else` para saltar (`pass`) los resultados de esta manera:\n",
    "\n",
    "Para las imágenes descartaremos aquellos enlaces que terminen en `.svg`\n",
    "Para los demás elementos, descartaremos las rutas que inicien con enlaces a notas [#], o a archivos [\"/wiki/Archivo:\"] o a enlaces externos [\"/w/\"]\n",
    "\n",
    "Finalmente, revisaremos que el resultado sea el que deseamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"https://es.wikipedia.org\"\n",
    "ruta = \"/wiki/Anexo:Pa%C3%ADses\" \n",
    "url = \"{}{}\".format(host,ruta)\n",
    "\n",
    "salsa = urllib.request.urlopen(url).read()\n",
    "sopa =  BeautifulSoup(salsa, 'html.parser')\n",
    "\n",
    "for contenido in sopa.select('table a'): # Aquí seleccionamos todos los enlaces en una tabla\n",
    "    obtener = \"{}{}\".format(host, contenido['href']) # Como ya sabemos, esta línea nos recuperará los enlaces\n",
    "    if obtener.endswith(\".svg\"):\n",
    "        pass\n",
    "    if contenido['href'].startswith(\"#\") or contenido['href'].startswith(\"/wiki/Archivo:\") or contenido['href'].startswith(\"/w/\"):\n",
    "        pass\n",
    "    else: # Extraemos el texto que será el nombre del enlace y una url completa.\n",
    "        print(contenido.text)\n",
    "        print(obtener)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado no es ideal, aún tenemos enlaces a páginas que no se refieren a países como \"Presidencialismo\" o \"Parlamentarismo\". Por lo pronto podemos pasar a resolver otros problemas y posteriormente nos encargaremos de estas páginas.\n",
    "\n",
    "Vamos a hacer una tabla que contenga las siguientes columnas: \"Nombres del país\", \"Población\" y el \"IDH\"\n",
    "\n",
    "En una página cualquiera de un país ubicamos la etiqueta con la cual extraer el nombre, que es bastante sencillo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://es.wikipedia.org/wiki/Afganist%C3%A1n\"\n",
    "\n",
    "salsa = urllib.request.urlopen(url).read()\n",
    "sopa =  BeautifulSoup(salsa, 'html.parser')\n",
    "\n",
    "titulo = sopa.select_one('h1', class_='firstHeading').text\n",
    "print(titulo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontrar el valor de la población es más complicado, debido a que no hay clases o identificadores en las celdas de las tablas de Wikipedia es necesario buscar por un texto clave. Para nuestro caso necesitamos que BeautifulSoup encuentre el texto en esta etiqueta:\n",
    "\n",
    "```html\n",
    "<td colspan=\"2\" style=\"border:0;padding:1px 7px 1px 1px;\">\n",
    "35 688 822&nbsp;hab.&nbsp;(2019)</td>```\n",
    "\n",
    "Después de probar diferentes claves, la única que dio resultado fue 'Censo'.\n",
    "\n",
    "Al recuperar el texto (\"35 688 822 hab. (2019)\") requerimos que nos muestre únicamente los dígitos y nos elimine la fecha. Para ello tuvimos que aplicar un loop en el que se dividiera el texto y se seleccionaran solamente los números, además de reemplazar los corchetes, las comas y los espacios. El resultado es como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celda = sopa.select_one('th:contains(Censo) + td').text\n",
    "numers = [int(s) for s in celda if s.isdigit()]\n",
    "stringnum = str(numers)\n",
    "numero = stringnum.replace('[', '').replace(']','').replace(',','').replace(' ','')\n",
    "censo = numero[:-4] # Con esta manipulación eliminamos los cuatro dígitos de la fecha\n",
    "\n",
    "print(censo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo la experiencia de la selección de la población podemos avanzar hacia la captura del valor del IDH, que es mucho más simple pues el valor numérico se puede obtener con una sencilla manipulación de la cadena de texto. Tuvimos que cambiar la coma por un punto en el separado para trabajar de manera más sencilla con el archivo csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celda2 = sopa.select_one('th:contains(IDH) + td').text\n",
    "idh = celda2[2:7].replace(',','.')\n",
    "\n",
    "print(idh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En conjunto, esto debería darnos el siguiente resultado en nuestra página de ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fila = \"{},{},{}\".format(titulo,censo,idh)\n",
    "\n",
    "print(fila)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como en los ejercicios anteriores, vamos a construir una función para facilitar la iteración sobre todos los elementos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wk_tabla(miurl):\n",
    "    \n",
    "    fila = []\n",
    "    \n",
    "    salsa = urllib.request.urlopen(miurl).read()\n",
    "    sopa =  BeautifulSoup(salsa, 'html.parser')\n",
    "\n",
    "    titulo = sopa.select_one('h1', class_='firstHeading').text\n",
    "    \n",
    "    celda = sopa.select_one('th:contains(Censo) + td').text\n",
    "    numers = [int(s) for s in celda if s.isdigit()]\n",
    "    stringnum = str(numers)\n",
    "    numero = stringnum.replace('[', '').replace(']','').replace(',','').replace(' ','')\n",
    "    censo = numero[:-4]\n",
    "\n",
    "    celda2 = sopa.select_one('th:contains(IDH) + td').text\n",
    "    idh = celda2[2:7].replace(',','.')\n",
    "    \n",
    "    fila.append(titulo)\n",
    "    fila.append(censo)\n",
    "    fila.append(idh)\n",
    "    \n",
    "    return fila"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos nuestra función ya podemos iterar únicamente sobre los enlaces a países, pues solamente estos contienen la tabla de información geográfica desde la cual obtendremos la información de la población y del IDH. \n",
    "En este caso no utilizaremos un condicional, sino aplicaremos un estilo de Python: 'Easier to ask for forgiveness than permission'. Es decir, intentaremos (`try`) que nos recupere la información de nuestra función, si esto no sucede nos marcará una excepción (`AttributeError`), el cual dejaremos pasar para que continue el loop. De esta manera, solamente la información de los países será recuperada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"https://es.wikipedia.org\"\n",
    "ruta = \"/wiki/Anexo:Pa%C3%ADses\" \n",
    "url = \"{}{}\".format(host,ruta)\n",
    "\n",
    "salsa = urllib.request.urlopen(url).read()\n",
    "sopa =  BeautifulSoup(salsa, 'html.parser')\n",
    "\n",
    "print(['País', 'Población', 'IDH'])\n",
    "\n",
    "for contenido in sopa.select('table a'): # Aquí seleccionamos todos los enlaces en una tabla\n",
    "    obtener = \"{}{}\".format(host, contenido['href']) # Como ya sabemos, esta línea nos recuperará los enlaces\n",
    "    if obtener.endswith(\".svg\"):\n",
    "        pass\n",
    "    if contenido['href'].startswith(\"#\") or contenido['href'].startswith(\"/wiki/Archivo:\") or contenido['href'].startswith(\"/w/\"):\n",
    "        pass\n",
    "    else:\n",
    "        try:\n",
    "            fila = wk_tabla(obtener)\n",
    "            print(fila)\n",
    "        except AttributeError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, para guardar un archivo `csv` solamente debemos hacer unos pequeños cambios al código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import csv # Esta librería nos ayudará a crear un archivo csv.\n",
    "\n",
    "path = os.getcwd()\n",
    "directorio = '{}/csv/'.format(path) # Cambiamos un poco la ruta del directorio para distinguirla del ejercicio anterior\n",
    "\n",
    "if not os.path.exists(directorio): # Creamos el directorio en caso de que no exista\n",
    "    os.makedirs(directorio)\n",
    "\n",
    "# Escribimos el encabezado de la tabla.\n",
    "with codecs.open('{}csv_paises.csv'.format(directorio), encoding=\"utf-8\", mode='a') as csvfile:\n",
    "    csvfile = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    csvfile.writerow(['País', 'Población', 'IDH'])\n",
    "    \n",
    "for contenido in sopa.select('table a'): # Aquí seleccionamos todos los enlaces en una tabla\n",
    "    obtener = \"{}{}\".format(host, contenido['href']) # Como ya sabemos, esta línea nos recuperará los enlaces\n",
    "    if obtener.endswith(\".svg\"):\n",
    "        pass\n",
    "    if contenido['href'].startswith(\"#\") or contenido['href'].startswith(\"/wiki/Archivo:\") or contenido['href'].startswith(\"/w/\"):\n",
    "        pass\n",
    "    else: # Extraemos el texto que será el nombre del enlace y una url completa.\n",
    "        try:\n",
    "            fila = wk_tabla(obtener)\n",
    "            # Escribimos cada fila de la tabla con los resultados que nos dará nuestra función.\n",
    "            with codecs.open('{}csv_paises.csv'.format(directorio), encoding=\"utf-8\", mode='a') as csvfile:\n",
    "                csvfile = csv.writer(csvfile, delimiter=',')\n",
    "                csvfile.writerow(fila)\n",
    "                \n",
    "        except AttributeError:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final\n",
    "\n",
    "Con este ejercicio finalizamos nuestro taller. Muchas líneas de este código se elaboraron específicamente para este taller y pueden ser reutilizadas, modificadas y mejoradas. Espero que pueda ser de utilidad para sus investigaciones. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "WS_Wikipedia.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
